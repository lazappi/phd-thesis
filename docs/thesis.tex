% *============================================================================*
%                  UNIVERSITY OF MELBOURNE THESIS TEMPLATE
% *============================================================================*

% +----------------------------------------------------------------------------+
%  This is a Pandoc/LaTeX template for a University of Melbourne thesis designed
%  to be used as part of a bookdown project
%  (https://bookdown.org/yihui/bookdown/).
% +----------------------------------------------------------------------------+

% +----------------------------------------------------------------------------+
%  Changes:
%
%  2018-10-10 (Luke Zappia)
%    * Create first draft template
% +----------------------------------------------------------------------------+

% *============================================================================*
%  PREAMBLE
%
%  Set up the document, load packages, set parameters etc.
% *============================================================================*

% +-----Document class---------------------------------------------------------+
%  Set the class associated with the document, options in square brackets are
%  passed to the class
% +----------------------------------------------------------------------------+

\documentclass[11pt,a4paper,titlepage,twoside,openright]{style/unimelbthesis}

% +-----Packages---------------------------------------------------------------+
%  External packages used in the document
% +----------------------------------------------------------------------------+

\usepackage{amsmath}   % American Mathematics Society standards
\usepackage{amsxtra}   % Additional math symbols
\usepackage{amssymb}   % Additional math symbols
\usepackage{amsthm}    % Additional math symbols
\usepackage{latexsym}  % Additional math symbols
\usepackage{booktabs}  % Table formatting
\usepackage{longtable} % Table formatting
\usepackage{hyperref}  % Hyperlinks
\usepackage{setspace}  % Line spacing
\usepackage{chemarr}   % Improved reaction arrows for chemists
\usepackage{palatino}  % Use the palatino font family
\usepackage{mathpazo}  % Use the palotino font family
\usepackage{graphicx}  % Extended graphics package

% +-----Parameters-------------------------------------------------------------+
%  Set parameters for the document. To convert from YAML to LaTeX we need to add
%  the dollar signs.
% +----------------------------------------------------------------------------+

\title{Tools and techniques for single-cell RNA-seq data}
\author{Luke Zappia}
\orcid{0000-0001-7744-8565}
\degree{Doctor of Philosophy}
\submissionmonth{November}
\submissionyear{2018}
\department{School of Biosciences}
\university{The University of Melbourne}
\statement{Submitted in Total Fulfillment of the Requirements of the Degree of Doctor of Philosophy}

% +-----Syntax highlighting----------------------------------------------------+
%  If code chunks are included in the document this allows Pandoc to insert the
%  code highlighting macros.
% +----------------------------------------------------------------------------+


% *============================================================================*
%  FRONT MATTER
%
%  Front section of the document, everything before the main text.
% *============================================================================*

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}

\begin{frontmatter}

% +-----Title------------------------------------------------------------------+

  \maketitle

% +-----Abstract---------------------------------------------------------------+

  \begin{abstract}
    The preface pretty much says it all.
    
    \par
    
    Second paragraph of abstract starts here.
  \end{abstract}

% +-----Declaration------------------------------------------------------------+

  \begin{declaration}
    This is to certify that:
    
    \begin{enumerate}
    \def\labelenumi{\roman{enumi}.}
    \tightlist
    \item
      the thesis comprises only their original work towards the {[}name of the award{]} except where indicated in the preface;
    \item
      due acknowledgement has been made in the text to all other material used; and
    \item
      the thesis is fewer than the maximum word limit in length, exclusive of tables, maps, bibliographies and appendices or that the thesis is {[}number of words{]} as approved by the Research Higher Degrees Committee.
    \end{enumerate}
  \end{declaration}

% +-----Preface----------------------------------------------------------------+

  \begin{preface}
    This is an example of a thesis setup to use the reed thesis document class (for LaTeX) and the R bookdown package, in general.
  \end{preface}

% +-----Acknowledgements-------------------------------------------------------+

  \begin{acknowledgements}
    This template is based on thesisdown (\url{https://github.com/ismayc/thesisdown}) and makes use of RMarkdown (\url{https://rmarkdown.rstudio.com/}) and bookdown \url{https://bookdown.org/yihui/bookdown/}. The LaTeX template is based on John Papandriopoulos' University of Melbourne thesis template (\url{https://github.com/jpap/phd-thesis-template}). Inspriation also comes from similar projects including beaverdown (\url{https://github.com/zkamvar/beaverdown}), aggidown (\url{https://github.com/ryanpeek/aggiedown}), huskydown (\url{https://github.com/benmarwick/huskydown}) and jayhawkdown (\url{https://github.com/wjakethompson/jayhawkdown}).
  \end{acknowledgements}

% +-----Table of contents------------------------------------------------------+

  \hypersetup{linkcolor=black}
  \setcounter{tocdepth}{2}
  \tableofcontents

% +-----List of tables---------------------------------------------------------+

  \listoftables

% +-----List of figures--------------------------------------------------------+

  \listoffigures

% +-----List of copyright------------------------------------------------------+


\end{frontmatter}

% *============================================================================*
%  MAIN MATTER
%
%  Main part of the document. Includes all chapters and appendices.
% *============================================================================*


\begin{mainmatter}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{rna-sequencing}{%
\section{RNA sequencing}\label{rna-sequencing}}

\begin{itemize}
\tightlist
\item
  Central dogma
\item
  Why RNA-seq?
\end{itemize}

\hypertarget{capture-and-reverse-transcription}{%
\subsection{Capture and reverse transcription}\label{capture-and-reverse-transcription}}

\begin{itemize}
\tightlist
\item
  PolyA capture
\item
  Ribosomal depletion
\end{itemize}

\hypertarget{high-throughput-sequencing}{%
\subsection{High-throughput sequencing}\label{high-throughput-sequencing}}

\begin{itemize}
\tightlist
\item
  Illumina sequencing

  \begin{itemize}
  \tightlist
  \item
    Sequence by synthesis
  \end{itemize}
\end{itemize}

\hypertarget{analysis-of-rna-seq-data}{%
\subsection{Analysis of RNA-seq data}\label{analysis-of-rna-seq-data}}

\begin{itemize}
\tightlist
\item
  Experimental design
\item
  Negative binomial
\item
  Normalisation
\item
  Differential expression testing
\end{itemize}

\hypertarget{single-cell-capture-technologies}{%
\section{Single-cell capture technologies}\label{single-cell-capture-technologies}}

\begin{itemize}
\tightlist
\item
  First protocol
\item
  Fluidigm
\end{itemize}

The first scRNA-seq protocol was published in 2009, just a year after the first bulk RNA-seq publication. While this approach allowed measurements of the transcriptome in individual cells it required manual manipulation and was restricted to inspecting a few precious cells. Further studies quickly showed that cell types could be identified without sorting cells and approaches were developed to allow unbiased capture of the whole transcriptome. Since then many scRNA-seq protocols have been developed including \ldots{}. The first commercially available cell capture platform was the Fluidigm C1. This system uses microfluidics to passively separate cells into individual wells on a plate where they are lysed, reverse-transcribed and the collected cDNA is PCR amplified. After this stage the product is extracted from the plate and libraries prepared for Illumina sequencing. Most C1 data has been produced using a 96 well plate but more recently an 800 well plate has become available, greatly increasing the number of cells that can be captured at a time. One of the disadvantages of microfluidic cell capture technologies is that the chips have a fixed size window, meaning that only cells of a particular sizes can be captured in a single run. However, as cells are captured in individual wells they can be imaged before lysis, potentially identifying damaged or broken cells, empty wells or wells containing more than one cell. Capturing multiple cells is a known issue, with Macosko et al.~finding that when preparing a mixture of mouse and human cells 30 percent of the resulting libraries contained transcripts from both species but only about a third of these doublets were visible in microscopy images{[}Macosko2015-rl{]}. The newer Polaris system from Fluidigm also uses microfluidics to capture cells but can select particular cells based on staining or fluorescent reporter expression and then hold them for up to 24 hours while introducing various stimuli. The cells can be imaged during this time before being lysed and prepared for RNA sequencing. This platform provides opportunities for a range of experiments that aren't possible using other capture technologies.

\hypertarget{droplet-based-cell-capture}{%
\subsection{Droplet based cell capture}\label{droplet-based-cell-capture}}

\begin{itemize}
\tightlist
\item
  Drop-seq
\item
  Indrop
\item
  10x Chromium
\end{itemize}

An alternative to using microfludics to capture cells in wells is to capture them in nano-droplets. A dissociated cell mixture is fed into a microfluidic device while at another input beads coated in primers enter. The device is designed to form aqueous droplets within mineral and the inputs are arranged so that cells and beads can be simultaneously captured within a droplet. When this happens the reagents carried along with the bead lyse the cell and any PolyA tagged RNA molecules present can bind to the capture probes on the bead. Reverse transcription and PCR amplification then begins and an individual cDNA library is produced for each cell, tagged with the unique barcode sequence present on the bead. The main advantage of droplet-based capture technologies is the ability to capture many more cells at one time, up to tens of thousands. These approaches are also less selective about cell size and produce less doublets. As a result they are much cheaper per a cell, although as sequencing costs are fixed studies using droplet-based captures typically sequence individual cells at a much lower depth.

Droplet-based capture was popularised by the publication of the Drop-seq and InDrop platforms in 2015. This are both DIY systems and although they differ in how the beads are produced, when the droplets are broken and some aspects of the chemistry they can both be constructed on a lab bench from syringes, automatic plungers, a micro scope and a small custom-made microfluidic chip. A similar commercially available platform is the 10x Genomics Chromium device which automates and streamlines much of the process. This device uses droplet-based technologies for a range of applications including capture of cells for scRNA-seq. More specialised captures, such as those aimed at profiling immune cell receptors are also possible and the company has recently announced kits for single-cell ATAC-seq capture.

\hypertarget{unique-molecular-identifiers}{%
\subsection{Unique Molecular Identifiers}\label{unique-molecular-identifiers}}

\begin{itemize}
\tightlist
\item
  Why?
\item
  How they work
\end{itemize}

In contrast to plate-based capture methods, which often provide reads along the length of transcripts, droplet-based capture methods typically employ protocols which include short random nucleotide sequences known as Unique Molecular Identifiers (UMIs). Individual cells contain very small amounts of RNA and to obtain enough cDNA a PCR amplification step is necessary. Depending on their nucleotide sequence different transcripts may be amplified at different rates which can distort their relative proportions within a library. UMIs attempt to improve the quantification of gene expression by allowing the removal of PCR duplicates produced during amplification. The nucleotide probes used in droplet-based capture protocols include a PolyT sequence which binds to mature mRNA molecules, a barcode sequence which is the same for every probe on a bead and 8-10 bases of UMI sequence which is unique to each probe. The UMI sequences are long enough that the probability of capturing two copies of a transcript on two probes with the same UMI is extremely low. After reverse-transcription, amplification, sequencing and alignment de-duplication can be performed by identifying reads with the same UMI that align to the same position and therefore should be PCR duplicates rather than truly expressed copies of a transcript. For this method to be effective each read must be associated with a UMI which means that only a small section at the 3' end of each transcript is sequenced. This has the side effect of reducing the amount of cDNA that needs to be sequenced and therefore increasing the number of cells that can be sequenced at a time. While the improvement in quantification of gene expression levels is useful for many downstream analyses it comes at the cost of coverage across the length of a gene which is required for applications such as variant detection and de-novo assembly. Statistical methods designed for full-length data may also be affected by the difference properties of a UMI dataset. Datasets with UMIs also need extra processing steps which can be complicated by the possibility of sequencing errors in the UMI itself.

\hypertarget{recent-advances}{%
\subsection{Recent advances}\label{recent-advances}}

\begin{itemize}
\tightlist
\item
  New capture methods
\item
  CITE-seq
\item
  Cell hashing
\item
  CRISPR
\item
  Multiple measurements, same cell
\end{itemize}

Although droplet-based techniques are currently the most commonly used cell capture technologies other approaches have been proposed that promise to capture even more cells even more cheaply. These include approaches based around nanowells\ldots{}

Extensions to the standard protocols have also been proposed that allow extra measurements from the same cell. One such protocol is CITE-seq which enables measurement of the levels of selected proteins at the same time as the whole transcriptome. Antibodies for the proteins of interest are labelled with short nucleotide sequences. These antibodies can then be applied to the dissociated cells and any that remain unbound washed away before cell capture. The antibody labels are then captured along with mRNA transcripts and a size selection step is applied to separate them before library preparation. Similar antibodies can be used to allow multiplexing of samples through a process known as cell hashing. In a typical scRNA-seq experiment each batch corresponds to a single sample. This complicated analysis as it is impossible to tell what is noise due to cells being processed in the same way and what is true biological signal. Cell hashing uses an antibody to a ubiquitously expressed protein but with a different nucleotide sequence for each sample. The samples can then be mixed, processed in batches and then the cells computationally separated based on which sequence they are associated with. An added benefit of this approach is the simple detection of doublets containing cells from different samples.

CRISPR-Cas9 gene editing has also been developed as an extension to scRNA-seq protocols. One possibility is to introduce a mutation at a known location that can then be used to demultiplex samples processed together. It is possible to do this with samples from different individuals or cell lines but the advantage of a gene editing based approach is that the genetic background remains similar between samples. It is also possible to investigate the effects of introducing a mutation. Protocols like Perturb-Seq introduce a range of guide RNA molecules to a cell culture, subject the cells to some stimulus then perform single-cell RNA sequencing. The introduced mutation can then be linked to the response of the cells to the stimulus and the associated broader changes in gene expression.

Other approaches that allow multiple measurements from individual cells include\ldots{}

\hypertarget{features-of-single-cell-rna-seq-data}{%
\section{Features of single-cell RNA-seq data}\label{features-of-single-cell-rna-seq-data}}

\begin{itemize}
\tightlist
\item
  Why use single-cell?
\item
  Low counts

  \begin{itemize}
  \tightlist
  \item
    Dropout
  \item
    Bursting
  \item
    Biology
  \end{itemize}
\end{itemize}

\hypertarget{analysing-scrna-seq-data}{%
\section{Analysing scRNA-seq data}\label{analysing-scrna-seq-data}}

\begin{verbatim}
Single-cell RNA-seq technologies have developed rapidly but there are still a
number of challenges. Existing protocols are inefficient, capturing only around
10 percent of transcripts in a cell[@Grun2014-zn]. Combined with the relatively
low sequencing depth that is commonly used, this results in a limited
sensitivity and an inability to reliably detect lowly expressed transcripts. The
small amount of starting material also contributes to high levels of technical
noise, complicating downstream analysis and making it difficult to detect
biological differences[@Liu2016-wq]. All high-throughput scRNA-seq protocols and
platforms require tissues to be dissociated into single-cell suspensions before
capture. This step can be non-trivial. Some tissues or cell types may not
readily separate and the treatments used to break them down may effect the
health of the cells and therefore their transcriptional profiles. Additionally,
some cell types may be too big or have other physical characteristics that
prevent them being captured using currect methods. Cells may also be damaged
during processing, or missing or multiple cells may have been sequenced, making
quality control an important consideration.

As well as introducing technical noise, the small amounts of starting material
and low sequencing depth mean there are many occasions  where there is no
measured expression for a particular gene in a particular cell. Some of these
zero counts are due to the biology we wish to study, for example we expect
different cell types to express different genes, but there are additional
biological factors such as the cell cycle, transcriptional bursting and
environmental interactions which cause genuine differences in expression between
cells performing the same function. On top on this are the technical effects
that have already been discussed including that  existing protocols may not
reliably capture all the RNA present, resulting in "dropout" events where a gene
is expressed in a sample but not observed in the sequencing data. These zeros
can make analysis difficult as methods must account for the missing information
and they may violate the assumptions of existing approaches. For example the
DEseq2 package[@Love2014-tw] has proven successful for detecting differential
expression in bulk RNA-seq but it relies on the presence of genes without any
zero counts, and therefore typically fails on scRNA-seq data. Examples of
scRNA-seq analysis packages that tackle this problem are Zero Inflated Factor
Analysis (ZIFA)[@Pierson2015-qp], which explicitly models dropout as it affects
dimensionality reduction, and Clustering through Imputation and Dimensionality
Reduction (CIDR)[@Lin2016-yu], which implicitly imputes zeros as it clusters
cells.

Bulk RNA-seq experiments are usually conducted on predefined groups of samples,
for example cancer cells and normal tissue, different tissue types or treated
and control cells. Some scRNA-seq experiments are done in a similar way where
cells are sorted into known types based on surface markers or selected at a
series of time points, but often they are more exploratory. Many of the current
studies have taken samples of developing or mature tissues and attempted to
identify what cell types are present[@Zeisel2015-rd; @Patel2014-bl;
@Treutlein2014-wd; @Usoskin2015-fz; @Buettner2015-rq; @Klein2015-iw;
@Trapnell2014-he]. This requires a new set of analysis techniques to be
developed which attempt to identify cell types. Success at this task is crucial
to the reliability of results from more developed analysis methods such as gene
testing. Additionally is it impossible to truly replicate an individual cell and
therefore thought must be given as to what constitutes a replicate for the
purposes of statistical analysis.
\end{verbatim}

\hypertarget{pre-processing-and-quality-control}{%
\subsection{Pre-processing and quality control}\label{pre-processing-and-quality-control}}

\begin{itemize}
\tightlist
\item
  Alignment
\item
  Droplet selection
\item
  UMIs
\item
  Doublet detection
\item
  Bad cells
\item
  Gene filtering
\item
  Cell ranger
\item
  scater
\end{itemize}

\begin{verbatim}
As scRNA-seq data has become available there has been a rapid development of new
bioinformatics tools attempting to unlock its potential. Currently there are at
least 80 available software packages that have been designed specifically for
the analysis of scRNA-seq data, the majority of which have been published in
peer-reviewed journals or as preprints. A table of scRNA-seq software is
available at [https://goo.gl/4wcVwn](). Prior to analysis the sequencing reads
from an scRNA-seq experiment are processed in much the same way as a bulk
experiment. Typically there is some quality control of the raw reads, reads are
aligned to a reference genome and the number of reads overlapping annotated
features (genes or transcripts) is counted. Alternatively, for full-length
reads, probabilistic quantification methods such as kallisto[@Bray2016-tm] or
Salmon[@Patro2015-kl] can be used. These approaches can greatly  improve
processing time which is important when there may be tens of thousands of
samples but they are currently incompatible with UMI protocols. When using
conventional alignment UMI samples need extra processing with tools such as
UMI-tools[@Smith2016-bt] or umis[@Svensson2016-eg]. The resulting gene by cell
matrix of expression values is the starting point for most analysis.

Quality control of individual cells is important as most experiments will
contain some low-quality cells that could be uninformative or misleading.
Quality control can be performed on various levels: on the quality scores of the
reads themselves, how or where the reads align or features of the expression
matrix such as the total expression, expression of spike-ins or expression of
particular genes. The Cellity package attempts to do this by inspecting a series
of biological and technical features and using principal component analysis or
machine learning methods to distinguish between high and low-quality
cells[@Ilicic2016-wy]. However the authors found that many of the features were
cell type specific and more work needs to be done to make this approach more
generally applicable. Jiang, Thomson and Stewart take a different approach,
assuming that expression outliers are associated with poor sequencing
quality[@Jiang2016-ys]. The scater package[@McCarthy2016-cw] emphasises a more
exploratory approach to quality control. While it cannot automatically detect
low-quality cells, scater provides a convenient object for storing scRNA-seq
data with functions for plotting associated features, making it easy for the
user to define their own filtering thresholds. Plate-based platforms such as the
Fluidigm C1 can have additional biases based on the location of individual
wells. The OEFinder package attempts to identify and visualise these "ordering
effects"[@Leng2016-it].
\end{verbatim}

\hypertarget{integrating-multiple-datasets}{%
\subsection{Integrating multiple datasets}\label{integrating-multiple-datasets}}

\begin{itemize}
\tightlist
\item
  Why?
\item
  Seurat CCA
\item
  New methods
\end{itemize}

\begin{verbatim}
Technical variation is a known problem in high-throughput studies and Kim et al.
predict that only 17.8 percent of allele-specific expression is due to
biological variation with the rest being technical noise[@Kim2015-mo]. Effective
normalisation has been shown to be a crucial aspect of analysis for bulk RNA-seq
datasets, but how normalisation should be be applied to single-cell datasets is
yet to be clearly established. Some studies use simple transformations like
Reads (or Fragments) Per Kilobase per Million (RPKM/FPKM)[@Mortazavi2008-vu] or
Transcripts Per Million (TPM)[@Wagner2012-qf] which correct for library size and
gene length. Alternatively, normalisation methods designed for detecting
differential expression in bulk samples such as the Trimmed Mean of M-Values
(TMM)[@Robinson2010-ll] or the DESeq method[@Anders2010-pq] can be applied, but
it is unclear how suitable they are for the single-cell context. Most of the
methods that have been developed specifically for estimating technical variance
in scRNA-seq data make use of spike-ins. Brennecke et al.[@Brennecke2013-pt],
Ding et al.[@Ding2015-ht] and Gr√ºn, Kester and van Oudenaarden[@Grun2014-zn] all
propose methods for estimating technical variance using spike-ins, as does
Bayesian Analysis of Single-Cell Sequencing data (BASiCS)[@Vallejos2015-ef].
However, using spike-ins for normalisation relys on the assumption that they
properly capture the dynamics of the underlying datasets, and even if this is
the case is it restrictive as they are not compatible with all current
sequencing protocols. Lun, Bach and Marioni don't make use of spike-ins, instead
using a pooling approach to compensate for the large number of zero counts,
where expression levels are summed across similar cells before calculating
normalisation factors that are deconvolved back to the individual cell
level[@Lun2016-mq].
\end{verbatim}

\hypertarget{grouping-cells}{%
\subsection{Grouping cells}\label{grouping-cells}}

\begin{itemize}
\tightlist
\item
  Clustering
\item
  Seurat
\item
  Other approaches
\end{itemize}

\begin{verbatim}
Once a set of high-quality cells has been established the true analysis can
begin. Many of the current packages focus on the task of assigning cells to
groups before applying more traditional differential expression testing. This
approach makes sense for a sample with a defined set of mature cell types and is
taken by tools such as SINgle CEll RNA-seq profiling Analysis
(SINCERA)[@Guo2015-mf], Single-Cell Consensus Clustering (SC3)[@Kiselev2016-fa],
Seurat[@Satija2015-or], single-cell latent variable model
(scLVM)[@Buettner2015-rq] and Spanning-tree Progression Analysis of
Density-normalised Events (SPADE)[@Anchang2016-vo], as well as BackSPIN which
was used to identify nine cell types and 47 distinct subclasses in the mouse
cortex and hippocampus[@Zeisel2015-rd]. These tools attempt to cluster similar
cells together based on their expression profiles, forming groups of cells of
the same type. Often a dimensionality reduction step is included which can help
to remove some of the noise present in scRNA-seq data. Once groups of cells are
identified many of these packages can test genes for changes in expression,
identifying genes that are differentially expressed across the groups, or marker
genes that are expressed in a single group. These genes can be used to identify
which cell types each group represents, or alternatively known marker genes can
be inspected.
\end{verbatim}

\hypertarget{ordering-cells}{%
\subsection{Ordering cells}\label{ordering-cells}}

\begin{itemize}
\tightlist
\item
  Pseudotime
\item
  Monocle
\item
  Other approaches
\end{itemize}

\begin{verbatim}
In other situations, for example where stem cells are differentiating into
mature cell types, it may be more appropriate to order cells along a continuous
trajectory from one cell type to another. Trajectory analysis was pioneered by
Monocle which used dimensionality reduction and computation of a minimum
spanning tree to explore a model of skeletal muscle
differentiation[@Trapnell2014-he]. Since then the Monocle algorithm has been
updated and a range of others developed, including TSCAN[@Ji2016-ws],
SLICER[@Welch2016-cw], CellTree[@DuVerle2016-ni], Sincell[@Julia2015-zc] and
Mpath[@Chen2016-kx]. In their review of methods for trajectory inference,
Cannoodt, Saelens and Saeys break the process into two parts, dimensionality
reduction and then trajectory modelling (Figure
\\ref{fig:trajectory-inference})[@Cannoodt2016-iv]. Dimensionality reduction
consists of calculating similarities between cells, projecting onto lower
dimensions using manifold learning techniques such as PCA or
t-SNE[@Maaten2008-ne] then clustering cells or constructing a graph between
then. The trajectory is then formed by finding a path between cells and ordering
the cells along it. Once a path has been inferred important genes can be
identified by looking for those that change expression over the course of the
path. These genes can be important in their own right as they describe the
biology of the path, but that can also be used to identify cell types at the end
points of the path or where the path branches. Deciding which assignment
approach is most appropriate depends on the source of that data and the
questions you intend to ask. There are currently no studies comprehensively
comparing the performance of different methods for each approach.
\end{verbatim}

\hypertarget{gene-detection-and-interpretation}{%
\subsection{Gene detection and interpretation}\label{gene-detection-and-interpretation}}

\begin{itemize}
\tightlist
\item
  DE
\item
  Marker genes

  \begin{itemize}
  \tightlist
  \item
    Alternatives - Gini, classifiers
  \end{itemize}
\item
  Reviews
\item
  Classification
\end{itemize}

\hypertarget{kidney-development}{%
\section{Kidney development}\label{kidney-development}}

\hypertarget{structure-and-function}{%
\subsection{Structure and function}\label{structure-and-function}}

\begin{itemize}
\tightlist
\item
  Kidney structure
\item
  Nephron structure
\item
  Important cell types
\end{itemize}

\hypertarget{stages-of-development}{%
\subsection{Stages of development}\label{stages-of-development}}

\begin{itemize}
\tightlist
\item
  Lineage
\item
  Important genes
\end{itemize}

\hypertarget{growing-kidney-organoids}{%
\subsection{Growing kidney organoids}\label{growing-kidney-organoids}}

\begin{itemize}
\tightlist
\item
  Why?

  \begin{itemize}
  \tightlist
  \item
    Disease modelling
  \end{itemize}
\item
  Protocol
\item
  Growth factors
\item
  Characterisation
\item
  Reproducibility
\end{itemize}

\hypertarget{the-scrna-seq-tools-landscape}{%
\chapter{The scRNA-seq tools landscape}\label{the-scrna-seq-tools-landscape}}

\hypertarget{simulating-scrna-seq-data}{%
\chapter{Simulating scRNA-seq data}\label{simulating-scrna-seq-data}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\hypertarget{splatter-publication}{%
\section{Splatter publication}\label{splatter-publication}}

\frame{\includegraphics[page=1, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=2, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=3, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=4, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=5, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=6, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=7, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=8, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=9, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=10, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=11, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=12, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=13, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=14, width=\textwidth]{figures/splatter-paper-cropped}}
\frame{\includegraphics[page=15, width=\textwidth]{figures/splatter-paper-cropped}}

\hypertarget{visualising-clustering-across-resolutions}{%
\chapter{Visualising clustering across resolutions}\label{visualising-clustering-across-resolutions}}

\hypertarget{analysis-of-kidney-organoid-scrna-seq-data}{%
\chapter{Analysis of kidney organoid scRNA-seq data}\label{analysis-of-kidney-organoid-scrna-seq-data}}

\hypertarget{conclusion}{%
\chapter{Conclusion}\label{conclusion}}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}

\end{mainmatter}

\end{document}
